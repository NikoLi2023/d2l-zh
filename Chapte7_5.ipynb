{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "    if not torch.is_grad_enabled(): #如果是预测模式，这为False\n",
    "        X_hat=(X-moving_mean)/torch.sqrt(moving_var+eps)\n",
    "    else:\n",
    "        #分来那种情况讨论，如果X是全连接层，len(X)==2 ,如果作用于卷积层在非线性函数之前，这len(X)==4;\n",
    "        assert len(X) in (2,4)\n",
    "        if len(X)==2:\n",
    "            mean=torch.mean(X,dim=0,keepdim=True)\n",
    "            var=((X-mean)**2).mean(dim=0)\n",
    "            \n",
    "        else:\n",
    "            mean=torch.mean(X,dim=(0,2,3),keepdim=True)\n",
    "            var=((X-mean)**2).mean(dim=(0,2,3),keepdim=True)\n",
    "        \n",
    "        X_hat=(X-mean)/torch.sqrt(var+eps)\n",
    "             # 更新移动平均的均值和方差\n",
    "        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\n",
    "        moving_var = momentum * moving_var + (1.0 - momentum) * var\n",
    "    Y = gamma * X_hat + beta  # 缩放和移位\n",
    "    return Y, moving_mean.data, moving_var.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self,numfeature,numdim):\n",
    "        super().__init__()\n",
    "        if(numdim==2): #定义Gammma，eps,Moving Mean,Moving variance\n",
    "            shape=(1,numfeature)\n",
    "        else:\n",
    "            shape=(1,numfeature,1,1)\n",
    "        self.gamma=nn.parameter(torch.ones(shape))\n",
    "        self.beta=nn.parameter(torch.zeros(shape))\n",
    "        self.moving_mean=torch.zeros(shape)\n",
    "        self.moving_var=torch.ones(shape)\n",
    "    def forward(self,X):\n",
    "        if self.moving_mean.device!=X.device:\n",
    "            self.moving_mean.device=self.moving_mean.to(X.device)\n",
    "            self.moving_var.device=self.moving_var.to(X.device)\n",
    "        \n",
    "        Y, self.moving_mean, self.moving_var = batch_norm(\n",
    "            X, self.gamma, self.beta, self.moving_mean,\n",
    "            self.moving_var, eps=1e-5, momentum=0.9)\n",
    "        return Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Residual \n",
    "from torch.nn import functional as F\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, input_channels, num_channels,\n",
    "                 use_1x1conv=False, strides=1) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1=nn.Conv2d(input_channels,num_channels,kernel_size=3,stride=strides,padding=1)\n",
    "        self.conv2=nn.Conv2d(num_channels,num_channels,kernel_size=3,padding=1)\n",
    "\n",
    "        if use_1x1conv:\n",
    "            self.conv3=nn.Conv2d(input_channels,num_channels,kernel_size=1,stride=strides)\n",
    "        else:\n",
    "            self.conv3=None\n",
    "        \n",
    "        self.norm1=nn.BatchNorm2d(num_channels)\n",
    "        self.norm2=nn.BatchNorm2d(num_channels)\n",
    "    def forward(self,X):\n",
    "        F1=F.relu(self.norm1(self.conv1(X)))\n",
    "        F2=self.norm2(self.conv2(F1))\n",
    "        if self.conv3:\n",
    "            X=self.conv3(X)\n",
    "        F2+=X\n",
    "        return F.relu(F2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义一个ResNet\n",
    "b1=nn.Sequential(nn.Conv2d(1,64,kernel_size=7,padding=3,stride=2),nn.BatchNorm2d(64),nn.ReLU(),nn.MaxPool2d(kernel_size=3,stride=2,padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_block(input_channels,num_channels,num_residuals,isFirstBlock):\n",
    "    block=[]\n",
    "    for i in range(num_residuals):\n",
    "        if i==0 and not isFirstBlock:\n",
    "            block.append(Residual(input_channels,num_channels,True,strides=2))\n",
    "        else:\n",
    "            block.append(Residual(num_channels,num_channels))\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=nn.Sequential(b1,nn.Sequential(*resnet_block(64,64,2,isFirstBlock=True)),\n",
    "                  nn.Sequential(*resnet_block(64,128,2,isFirstBlock=False)),\n",
    "                  nn.Sequential(*resnet_block(128,256,2,isFirstBlock=False)),\n",
    "                  nn.Sequential(*resnet_block(256,512,2,isFirstBlock=False)),\n",
    "                  nn.AdaptiveAvgPool2d((1,1)),nn.Flatten(),nn.Linear(512,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential OutPutShape: torch.Size([1, 64, 56, 56])\n",
      "Sequential OutPutShape: torch.Size([1, 64, 56, 56])\n",
      "Sequential OutPutShape: torch.Size([1, 128, 28, 28])\n",
      "Sequential OutPutShape: torch.Size([1, 256, 14, 14])\n",
      "Sequential OutPutShape: torch.Size([1, 512, 7, 7])\n",
      "AdaptiveAvgPool2d OutPutShape: torch.Size([1, 512, 1, 1])\n",
      "Flatten OutPutShape: torch.Size([1, 512])\n",
      "Linear OutPutShape: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X=torch.rand(size=(1,1,224,224))\n",
    "for layer in net:\n",
    "    X=layer(X)\n",
    "    print(layer.__class__.__name__,\"OutPutShape:\",X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs, batch_size = 0.05, 10, 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)\n",
    "d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Niko",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
