{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "    if not torch.is_grad_enabled(): #如果是预测模式，这为False\n",
    "        X_hat=(X-moving_mean)/torch.sqrt(moving_var+eps)\n",
    "    else:\n",
    "        #分来那种情况讨论，如果X是全连接层，len(X)==2 ,如果作用于卷积层在非线性函数之前，这len(X)==4;\n",
    "        assert len(X) in (2,4)\n",
    "        if len(X)==2:\n",
    "            mean=torch.mean(X,dim=0,keepdim=True)\n",
    "            var=((X-mean)**2).mean(dim=0)\n",
    "            \n",
    "        else:\n",
    "            mean=torch.mean(X,dim=(0,2,3),keepdim=True)\n",
    "            var=((X-mean)**2).mean(dim=(0,2,3),keepdim=True)\n",
    "        \n",
    "        X_hat=(X-mean)/torch.sqrt(var+eps)\n",
    "             # 更新移动平均的均值和方差\n",
    "        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean\n",
    "        moving_var = momentum * moving_var + (1.0 - momentum) * var\n",
    "    Y = gamma * X_hat + beta  # 缩放和移位\n",
    "    return Y, moving_mean.data, moving_var.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self,numfeature,numdim):\n",
    "        super().__init__()\n",
    "        if(numdim==2): #定义Gammma，eps,Moving Mean,Moving variance\n",
    "            shape=(1,numfeature)\n",
    "        else:\n",
    "            shape=(1,numfeature,1,1)\n",
    "        self.gamma=nn.parameter(torch.ones(shape))\n",
    "        self.beta=nn.parameter(torch.zeros(shape))\n",
    "        self.moving_mean=torch.zeros(shape)\n",
    "        self.moving_var=torch.ones(shape)\n",
    "    def forward(self,X):\n",
    "        if self.moving_mean.device!=X.device:\n",
    "            self.moving_mean.device=self.moving_mean.to(X.device)\n",
    "            self.moving_var.device=self.moving_var.to(X.device)\n",
    "        \n",
    "        Y, self.moving_mean, self.moving_var = batch_norm(\n",
    "            X, self.gamma, self.beta, self.moving_mean,\n",
    "            self.moving_var, eps=1e-5, momentum=0.9)\n",
    "        return Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Residual \n",
    "from torch.nn import functional as F\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, input_channels, num_channels,\n",
    "                 use_1x1conv=False, strides=1) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1=nn.Conv2d(input_channels,num_channels,kernel_size=3,stride=strides,padding=1)\n",
    "        self.conv2=nn.Conv2d(num_channels,num_channels,kernel_size=3,padding=1)\n",
    "\n",
    "        if use_1x1conv:\n",
    "            self.conv3=nn.Conv2d(input_channels,num_channels,kernel_size=1,stride=strides)\n",
    "        else:\n",
    "            self.conv3=None\n",
    "        \n",
    "        self.norm1=nn.BatchNorm2d(num_channels)\n",
    "        self.norm2=nn.BatchNorm2d(num_channels)\n",
    "    def forward(self,X):\n",
    "        F1=F.relu(self.norm1(self.conv1(X)))\n",
    "        F2=self.norm2(self.conv2(F1))\n",
    "        if self.conv3:\n",
    "            X=self.conv3(X)\n",
    "        F2+=X\n",
    "        return F.relu(F2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义一个ResNet\n",
    "b1=nn.Sequential(nn.Conv2d(1,64,kernel_size=7,padding=3,stride=2),nn.BatchNorm2d(64),nn.ReLU(),nn.MaxPool2d(kernel_size=3,stride=2,padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_block(input_channels,num_channels,num_residuals,isFirstBlock):\n",
    "    block=[]\n",
    "    for i in range(num_residuals):\n",
    "        if i==0 and not isFirstBlock:\n",
    "            block.append(Residual(input_channels,num_channels,True,strides=2))\n",
    "        else:\n",
    "            block.append(Residual(num_channels,num_channels))\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=nn.Sequential(b1,nn.Sequential(*resnet_block(64,64,2,isFirstBlock=True)),\n",
    "                  nn.Sequential(*resnet_block(64,128,2,isFirstBlock=False)),\n",
    "                  nn.Sequential(*resnet_block(128,256,2,isFirstBlock=False)),\n",
    "                  nn.Sequential(*resnet_block(256,512,2,isFirstBlock=False)),\n",
    "                  nn.AdaptiveAvgPool2d((1,1)),nn.Flatten(),nn.Linear(512,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential OutPutShape: torch.Size([1, 64, 56, 56])\n",
      "Sequential OutPutShape: torch.Size([1, 64, 56, 56])\n",
      "Sequential OutPutShape: torch.Size([1, 128, 28, 28])\n",
      "Sequential OutPutShape: torch.Size([1, 256, 14, 14])\n",
      "Sequential OutPutShape: torch.Size([1, 512, 7, 7])\n",
      "AdaptiveAvgPool2d OutPutShape: torch.Size([1, 512, 1, 1])\n",
      "Flatten OutPutShape: torch.Size([1, 512])\n",
      "Linear OutPutShape: torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X=torch.rand(size=(1,1,224,224))\n",
    "for layer in net:\n",
    "    X=layer(X)\n",
    "    print(layer.__class__.__name__,\"OutPutShape:\",X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DenseNet 的思想\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0} {4}\n",
      "{1} {4}\n",
      "{2} {4}\n",
      "{3} {4}\n"
     ]
    }
   ],
   "source": [
    "A=[4,4,4,4]\n",
    "for i ,num in enumerate(A):\n",
    "    print({i},{num})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 23, 8, 8])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "\n",
    "def conv_block(input_channels, num_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.BatchNorm2d(input_channels), nn.ReLU(),\n",
    "        nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1))\n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, num_convs, input_channels, num_channels):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        layer = []\n",
    "        for i in range(num_convs):\n",
    "            layer.append(conv_block(\n",
    "                num_channels * i + input_channels, num_channels))\n",
    "        self.net = nn.Sequential(*layer)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for blk in self.net:\n",
    "            Y = blk(X)\n",
    "            # 连接通道维度上每个块的输入和输出\n",
    "            X = torch.cat((X, Y), dim=1)\n",
    "        return X\n",
    "\n",
    "blk = DenseBlock(2, 3, 10)\n",
    "X = torch.randn(4, 3, 8, 8)\n",
    "Y = blk(X)\n",
    "Y.shape\n",
    "\n",
    "b1 = nn.Sequential(\n",
    "    nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "    nn.BatchNorm2d(64), nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "\n",
    "def transition_block(input_channels, num_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.BatchNorm2d(input_channels), nn.ReLU(),\n",
    "        nn.Conv2d(input_channels, num_channels, kernel_size=1),\n",
    "        nn.AvgPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "# num_channels为当前的通道数\n",
    "num_channels, growth_rate = 64, 32\n",
    "num_convs_in_dense_blocks = [4, 4, 4, 4]\n",
    "blks = []\n",
    "for i, num_convs in enumerate(num_convs_in_dense_blocks):\n",
    "    blks.append(DenseBlock(num_convs, num_channels, growth_rate))\n",
    "    # 上一个稠密块的输出通道数\n",
    "    num_channels += num_convs * growth_rate\n",
    "    # 在稠密块之间添加一个转换层，使通道数量减半\n",
    "    if i != len(num_convs_in_dense_blocks) - 1:\n",
    "        blks.append(transition_block(num_channels, num_channels // 2))\n",
    "        num_channels = num_channels // 2\n",
    "\n",
    "net = nn.Sequential(\n",
    "    b1, *blks,\n",
    "    nn.BatchNorm2d(num_channels), nn.ReLU(),\n",
    "    nn.AdaptiveAvgPool2d((1, 1)),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(num_channels, 10))\n",
    "\n",
    "net = nn.Sequential(\n",
    "    b1, *blks,\n",
    "    nn.BatchNorm2d(num_channels), nn.ReLU(),\n",
    "    nn.AdaptiveAvgPool2d((1, 1)),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(num_channels, 10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Niko",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
